{"cells":[{"cell_type":"code","execution_count":13,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-06T20:29:11.610362Z","iopub.status.busy":"2024-10-06T20:29:11.609732Z","iopub.status.idle":"2024-10-06T20:29:11.971845Z","shell.execute_reply":"2024-10-06T20:29:11.971019Z","shell.execute_reply.started":"2024-10-06T20:29:11.610313Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["Recebendo inputs do kaggle e separando em pastas"]},{"cell_type":"markdown","metadata":{},"source":["Importando dependências"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T20:31:08.510036Z","iopub.status.busy":"2024-10-06T20:31:08.509594Z","iopub.status.idle":"2024-10-06T20:31:12.080329Z","shell.execute_reply":"2024-10-06T20:31:12.079496Z","shell.execute_reply.started":"2024-10-06T20:31:08.510004Z"},"trusted":true},"outputs":[],"source":["# Importe pacotes\n","\n","import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import random\n","import shutil"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Caminhos para as pastas\n","train_color_dir = \"data/original/train\"\n","train_label_dir = \"data/conditional/train_labels\"\n","val_color_dir = \"data/original/val\"\n","val_label_dir = \"data/conditional/val_labels\"\n","test_color_dir = \"data/original/test\"\n","test_label_dir = \"data/conditional/test_labels\""]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Listar imagens (assumindo que os pares têm o mesmo nome de arquivo)\n","train_color_images = sorted(os.listdir(train_color_dir))\n","val_color_images = sorted(os.listdir(val_color_dir))\n","test_color_images = sorted(os.listdir(test_color_dir))\n","\n","train_label_images = sorted(os.listdir(train_label_dir))\n","val_label_images = sorted(os.listdir(val_label_dir))\n","test_label_images = sorted(os.listdir(test_label_dir))\n","\n","# Extraindo nomes dos arquivos (sem a extensão)\n","train_names = [os.path.splitext(img)[0] for img in train_color_images]\n","val_names = [os.path.splitext(img)[0] for img in val_color_images]\n","test_names = [os.path.splitext(img)[0] for img in test_color_images]\n","\n","train_names_labels = [os.path.splitext(img)[0] for img in train_label_images]\n","val_names_labels = [os.path.splitext(img)[0] for img in val_label_images]\n","test_names_labels = [os.path.splitext(img)[0] for img in test_label_images]\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Preparando imagens"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T20:31:12.083043Z","iopub.status.busy":"2024-10-06T20:31:12.082615Z","iopub.status.idle":"2024-10-06T20:31:13.286862Z","shell.execute_reply":"2024-10-06T20:31:13.285939Z","shell.execute_reply.started":"2024-10-06T20:31:12.083010Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DataLoaders criados com sucesso!\n"]}],"source":["from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image\n","\n","# Definindo transformações para as imagens\n","transform = transforms.Compose([\n","    transforms.Resize((256, 256)),  # Redimensiona as imagens para um tamanho fixo\n","    transforms.ToTensor()  # Converte as imagens para tensores\n","])\n","\n","# Classe customizada para carregar as imagens\n","class CustomDataset(Dataset):\n","    def __init__(self, label_dir, color_dir, image_names, label_names, transform=None):\n","        self.label_dir = label_dir\n","        self.color_dir = color_dir\n","        self.image_names = image_names\n","        self.label_names = label_names\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        label_image_path = os.path.join(self.label_dir, f\"{self.label_names[idx]}.png\")\n","        color_image_path = os.path.join(self.color_dir, f\"{self.image_names[idx]}.png\")\n","\n","        label_image = Image.open(label_image_path).convert(\"RGB\")\n","        color_image = Image.open(color_image_path).convert(\"RGB\")\n","\n","        if self.transform:\n","            label_image = self.transform(label_image)\n","            color_image = self.transform(color_image)\n","\n","        return label_image, color_image\n","\n","# Criando datasets\n","train_dataset = CustomDataset(train_label_dir, train_color_dir, train_names, train_names_labels, transform=transform)\n","val_dataset = CustomDataset(val_label_dir, val_color_dir, val_names, val_names_labels, transform=transform)\n","test_dataset = CustomDataset(test_label_dir, test_color_dir, test_names, test_names_labels, transform=transform)\n","\n","# Criando DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)\n","val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)\n","\n","print(\"DataLoaders criados com sucesso!\")"]},{"cell_type":"markdown","metadata":{},"source":["Definindo Gerador"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T20:31:13.288774Z","iopub.status.busy":"2024-10-06T20:31:13.288351Z","iopub.status.idle":"2024-10-06T20:31:13.309908Z","shell.execute_reply":"2024-10-06T20:31:13.308937Z","shell.execute_reply.started":"2024-10-06T20:31:13.288741Z"},"trusted":true},"outputs":[],"source":["# U-Net\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class UNet(nn.Module):\n","    def __init__(self, image_dim, n_channels=64, depth=5, conv_kernel_size=3, conv_stride=1, conv_padding=1, pool_kernel_size=2, pool_stride=2, pool_padding=0, transpose_kernel_size=3, transpose_stride=2, transpose_padding=1):\n","        super(UNet, self).__init__()\n","\n","        self.image_dim = image_dim  # Dimensões da imagem de entrada (C, H, W)\n","        self.depth = depth \n","        self.n_channels = n_channels\n","        self.conv_kernel_size = conv_kernel_size\n","        self.conv_stride = conv_stride\n","        self.conv_padding = conv_padding\n","        self.pool_kernel_size = pool_kernel_size\n","        self.pool_stride = pool_stride\n","        self.pool_padding = pool_padding\n","        self.transpose_kernel_size = transpose_kernel_size\n","        self.transpose_stride = transpose_stride\n","        self.transpose_padding = transpose_padding\n","\n","        # Encoder\n","        self.encoders = nn.ModuleList([self.conv_block(3 if i == 0 else self.n_channels * (2 ** (i-1)), self.n_channels * (2 ** i)) for i in range(self.depth)])\n","        self.pool = nn.MaxPool2d(kernel_size=self.pool_kernel_size, stride=self.pool_stride, padding=self.pool_padding)\n","\n","        # Bottleneck\n","        self.bottleneck = self.conv_block(self.n_channels * (2 ** (self.depth-1)), self.n_channels * (2 ** self.depth))\n","\n","        # Decoder\n","        self.decoders = nn.ModuleList([self.conv_transpose(self.n_channels * (2 ** (i+2)), self.n_channels * (2 ** i)) for i in range(self.depth-2, -1, -1)])\n","\n","        # Final conv layer\n","        self.final_conv = nn.Conv2d(self.n_channels, 3, kernel_size=1)\n","\n","    def conv_block(self, in_channels, out_channels):\n","        # Camada convolucional com normalização e função de ativação; 2 vezes\n","        return nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=self.conv_kernel_size, stride=self.conv_stride, padding=self.conv_padding),\n","            nn.BatchNorm2d(out_channels),  # Normalização para acelerar o treinamento\n","            nn.ReLU(inplace=True),  # Função de ativação (zera os valores negativos)\n","            nn.Conv2d(out_channels, out_channels, kernel_size=self.conv_kernel_size, stride=self.conv_stride, padding=self.conv_padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def crop(self, encoder_feature, decoder_feature):\n","        _, _, h, w = decoder_feature.size()\n","        encoder_feature = F.interpolate(encoder_feature, size=(h, w), mode='bilinear', align_corners=False)  # Redimensiona a feature map do encoder\n","        return encoder_feature\n","\n","    def conv_transpose(self, in_channels, out_channels):\n","        return nn.Sequential(\n","            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=self.transpose_kernel_size, stride=self.transpose_stride, padding=self.transpose_padding),\n","            self.conv_block(out_channels, out_channels)\n","        )\n","    \n","    def forward(self, x):\n","        #print(f\"Input shape: {x.shape}\")\n","        encoders_features = []\n","\n","        # Encoder pass\n","        for idx, encoder in enumerate(self.encoders):\n","            #print(\"idx: \", idx)\n","            #print(\"encoder: \", encoder)\n","            x = encoder(x)\n","            encoders_features.append(x)\n","            #print(f\"After encoder block {idx+1}: {x.shape}\")\n","            x = self.pool(x)\n","            #print(f\"After pooling {idx+1}: {x.shape}\")\n","\n","        # Bottleneck\n","        x = self.bottleneck(x)\n","        #print(f\"After bottleneck: {x.shape}\")\n","        # Doubled the block\n","        #print(\"Starting decoder pass\")\n","        # Decoder pass\n","        for i, decoder in enumerate(self.decoders):\n","            #print(\"i: \", i)\n","            #print(\"decoder: \", decoder)\n","            encoder_feature = encoders_features[-(i+1)]\n","            encoder_feature = self.crop(encoder_feature, x)  # Aplica o crop nas feature maps\n","            #print(f\"Encoder feature {i+1} after crop: {encoder_feature.shape}\")\n","\n","            if i != 0:\n","                x = torch.cat([encoder_feature, x], dim=1)  # Concatena encoder com decoder\n","                #print(f\"After concatenation with encoder feature {i+1}: {x.shape}\")\n","\n","            x = decoder(x)\n","\n","        # Final convolution\n","        x = self.final_conv(x)\n","        x = F.interpolate(x, size=(self.image_dim[2], self.image_dim[3]), mode='bilinear', align_corners=False)\n","        #print(f\"Output shape after final convolution: {x.shape}\")\n","        \n","        x = torch.sigmoid(x)\n","        \n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["Testando gerador"]},{"cell_type":"markdown","metadata":{},"source":["Gerador gera uma imagem do tamanho desejado, passando pela unet"]},{"cell_type":"markdown","metadata":{},"source":["Discriminador"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T20:31:13.311747Z","iopub.status.busy":"2024-10-06T20:31:13.311326Z","iopub.status.idle":"2024-10-06T20:31:13.329316Z","shell.execute_reply":"2024-10-06T20:31:13.328505Z","shell.execute_reply.started":"2024-10-06T20:31:13.311715Z"},"trusted":true},"outputs":[],"source":["\n","class Discriminator(nn.Module):\n","    def __init__(self, image_dim, n_channels=64, depth=5, conv_kernel_size=3, conv_stride=1, conv_padding=1, pool_kernel_size=2, pool_stride=2, pool_padding=0, transpose_kernel_size=3, transpose_stride=2, transpose_padding=1):\n","        super().__init__()\n","\n","        self.image_dim = image_dim  # Dimensões da imagem de entrada (C, H, W)\n","        self.depth = depth \n","        self.n_channels = n_channels\n","        self.conv_kernel_size = conv_kernel_size\n","        self.conv_stride = conv_stride\n","        self.conv_padding = conv_padding\n","        self.pool_kernel_size = pool_kernel_size\n","        self.pool_stride = pool_stride\n","        self.pool_padding = pool_padding\n","        self.transpose_kernel_size = transpose_kernel_size\n","        self.transpose_stride = transpose_stride\n","        self.transpose_padding = transpose_padding\n","\n","        # Encoder\n","        self.encoders = nn.ModuleList([self.conv_block(3 if i == 0 else self.n_channels * (2 ** (i-1)), self.n_channels * (2 ** i)) for i in range(self.depth)])\n","        self.pool = nn.MaxPool2d(kernel_size=self.pool_kernel_size, stride=self.pool_stride, padding=self.pool_padding)\n","\n","        # Bottleneck\n","        self.bottleneck = self.conv_block(self.n_channels * (2 ** (self.depth-1)), self.n_channels * (2 ** self.depth))\n","\n","        # Final conv layer\n","        self.final_conv = nn.Conv2d(self.n_channels * (2 ** self.depth), 1, kernel_size=1)\n","\n","        # Fully connected layer to reduce the dimensions to [32, 1, 1, 1]\n","        self.fc = nn.Linear(self.image_dim[2] * self.image_dim[3], 1)\n","\n","    def conv_block(self, in_channels, out_channels):\n","        # Camada convolucional com normalização e função de ativação; 2 vezes\n","        return nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=self.conv_kernel_size, stride=self.conv_stride, padding=self.conv_padding),\n","            nn.BatchNorm2d(out_channels),  # Normalização para acelerar o treinamento\n","            nn.ReLU(inplace=True)  # Função de ativação (zera os valores negativos)\n","        )\n","    \n","        \"\"\"\n","            nn.Conv2d(out_channels, out_channels, kernel_size=self.conv_kernel_size, stride=self.conv_stride, padding=self.conv_padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        \"\"\"\n","\n","    def crop(self, encoder_feature, decoder_feature):\n","        _, _, h, w = decoder_feature.size()\n","        encoder_feature = F.interpolate(encoder_feature, size=(h, w), mode='bilinear', align_corners=False)  # Redimensiona a feature map do encoder\n","        return encoder_feature\n","\n","    def conv_transpose(self, in_channels, out_channels):\n","        return nn.Sequential(\n","            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=self.transpose_kernel_size, stride=self.transpose_stride, padding=self.transpose_padding),\n","            self.conv_block(out_channels, out_channels)\n","        )\n","    \n","    def forward(self, x):\n","        #print(f\"Input shape: {x.shape}\")\n","        encoders_features = []\n","\n","        # Encoder pass\n","        for idx, encoder in enumerate(self.encoders):\n","            #print(\"idx: \", idx)\n","            #print(\"encoder: \", encoder)\n","            x = encoder(x)\n","            encoders_features.append(x)\n","            #print(f\"After encoder block {idx+1}: {x.shape}\")\n","            x = self.pool(x)\n","            #print(f\"After pooling {idx+1}: {x.shape}\")\n","\n","        # Bottleneck\n","        x = self.bottleneck(x)\n","        #print(f\"After bottleneck: {x.shape}\")\n","        # Doubled the block\n","        #print(\"Starting decoder pass\")\n","\n","        # Final convolution\n","        x = self.final_conv(x)\n","        x = F.interpolate(x, size=(1, 1), mode='bilinear', align_corners=False)\n","        #print(f\"Output shape after final convolution: {x.shape}\")\n","        x = torch.sigmoid(x)\n","\n","\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["Treino"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T20:31:13.330777Z","iopub.status.busy":"2024-10-06T20:31:13.330411Z","iopub.status.idle":"2024-10-06T20:31:13.341744Z","shell.execute_reply":"2024-10-06T20:31:13.340878Z","shell.execute_reply.started":"2024-10-06T20:31:13.330732Z"},"trusted":true},"outputs":[],"source":["from torch.optim import Adam, SGD\n","from torch.nn import BCELoss, L1Loss, MSELoss\n","import time\n","\n","# Loss para o discriminador\n","d_loss = BCELoss()\n","\n","class GeneratorLoss(nn.Module):\n","    def __init__(self, lambda_l1=100):\n","        super(GeneratorLoss, self).__init__()\n","        # Definir a perda BCE (usada para a parte adversária)\n","        self.adversarial_loss = nn.MSELoss()\n","        # Definir a perda L1\n","        self.l1_loss = nn.L1Loss()\n","        # Peso da perda L1\n","        self.lambda_l1 = lambda_l1\n","\n","    def forward(self, yhat_fake, fake_images, real_images):\n","        \"\"\"\n","        Parâmetros:\n","        - yhat_fake: As previsões do discriminador sobre as imagens geradas (fake)\n","        - fake_images: Imagens geradas pelo gerador\n","        - real_images: Imagens reais de referência\n","        \n","        Retorna:\n","        - A perda total combinando perda adversária e L1\n","        \"\"\"\n","        # Perda adversária: tentar fazer o discriminador acreditar que as imagens geradas são reais\n","        adversarial_loss = self.adversarial_loss(yhat_fake, torch.ones_like(yhat_fake))\n","        \n","        # Perda L1: minimizar a diferença entre as imagens geradas e as reais\n","        l1_loss = self.l1_loss(fake_images, real_images)\n","        \n","        # Perda total: adversária + ponderação da perda L1\n","        total_loss = adversarial_loss + self.lambda_l1 * l1_loss\n","        \n","        return total_loss\n","\n","g_loss = GeneratorLoss()\n","#g_loss = CycleGANLoss()\n","\n","# Normalize the input values to be between 0 and 1\n","def normalize(tensor):\n","    return (tensor - tensor.min()) / (tensor.max() - tensor.min())\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T20:31:13.343419Z","iopub.status.busy":"2024-10-06T20:31:13.343011Z","iopub.status.idle":"2024-10-06T20:31:13.355137Z","shell.execute_reply":"2024-10-06T20:31:13.354308Z","shell.execute_reply.started":"2024-10-06T20:31:13.343372Z"},"trusted":true},"outputs":[],"source":["class PaisagemGAN(nn.Module):\n","    def __init__(self, generator, discriminator):\n","        super(PaisagemGAN, self).__init__()\n","        self.generator = generator\n","        self.discriminator = discriminator\n","        self.global_step = 0\n","\n","    def compile(self, g_loss, d_loss):\n","        self.g_loss = g_loss\n","        self.d_loss = d_loss\n","        self.g_opt = Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","        self.d_opt = SGD(self.discriminator.parameters(), lr=0.0002, momentum=0.0)\n","\n","    def train_step(self, real_images, grey_images):\n","        \n","        # Train the discriminator\n","        #inicio = time.time()\n","        self.d_opt.zero_grad()\n","        \n","        fake_images = self.generator(grey_images)\n","        yhat_real = self.discriminator(real_images)\n","        yhat_fake = self.discriminator(fake_images)\n","        \n","        # Suavização de rótulos para imagens reais (usando 0.9 ao invés de 1.0)\n","        d_loss_real = self.d_loss(yhat_real, torch.full_like(yhat_real, 0.9))\n","        # Suavização opcional de rótulos para imagens falsas (usando 0.1 ao invés de 0.0)\n","        d_loss_fake = self.d_loss(yhat_fake, torch.full_like(yhat_fake, 0.1))\n","        # Soma das perdas\n","        total_d_loss = d_loss_real + d_loss_fake\n","        total_d_loss.backward()\n","        \n","        # Update discriminator weights every 5 steps\n","        if self.global_step % 5 == 0:\n","            self.d_opt.step()\n","        \n","        self.global_step += 1\n","        #fim = time.time()\n","        #print(fim - inicio)\n","\n","        # Train the generator\n","        #inicio = time.time()\n","        self.g_opt.zero_grad()\n","        \n","        fake_images = self.generator(grey_images)\n","        yhat_fake = self.discriminator(fake_images)\n","        total_g_loss = self.g_loss(yhat_fake, fake_images, real_images)\n","        \n","        total_g_loss.backward()\n","        self.g_opt.step()\n","        #fim = time.time()\n","        #print(fim - inicio)\n","        \n","        return {'d_loss': total_d_loss.item(), 'g_loss': total_g_loss.item(), 'gen': fake_images}"]},{"cell_type":"markdown","metadata":{},"source":["Realizar treinamento"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T20:31:13.356586Z","iopub.status.busy":"2024-10-06T20:31:13.356247Z","iopub.status.idle":"2024-10-06T20:31:14.754833Z","shell.execute_reply":"2024-10-06T20:31:14.753735Z","shell.execute_reply.started":"2024-10-06T20:31:13.356532Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([16, 3, 256, 256])\n","Dispositivo: cuda\n","Dispositivo: cuda\n"]}],"source":["nex = next(iter(train_loader))\n","img_dim = nex[0].shape\n","print(img_dim)\n","\n","gene = UNet(image_dim=img_dim, n_channels=64, depth=5, conv_kernel_size=3, conv_stride=1, conv_padding=1, pool_kernel_size=2, pool_stride=2, pool_padding=0, transpose_kernel_size=3, transpose_stride=2, transpose_padding=1)\n","# Configurando o dispositivo (GPU, se disponível)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Dispositivo: {device}\")\n","gene = gene.to(device)  # Mover o modelo para o dispositivo\n","\n","disc = Discriminator(image_dim=img_dim, n_channels=64, depth=3, conv_kernel_size=3, conv_stride=1, conv_padding=1, pool_kernel_size=2, pool_stride=2, pool_padding=0, transpose_kernel_size=3, transpose_stride=2, transpose_padding=1)\n","# Configurando o dispositivo (GPU, se disponível)\n","#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Dispositivo: {device}\")\n","disc = disc.to(device)  # Mover o modelo para o dispositivo"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T20:31:14.756646Z","iopub.status.busy":"2024-10-06T20:31:14.756332Z","iopub.status.idle":"2024-10-06T20:31:14.762232Z","shell.execute_reply":"2024-10-06T20:31:14.761282Z","shell.execute_reply.started":"2024-10-06T20:31:14.756614Z"},"trusted":true},"outputs":[],"source":["def save_checkpoint(model, optimizer, epoch, batch, loss, filepath='checkpoint.pth'):\n","    checkpoint = {\n","        'epoch': epoch,\n","        'batch': batch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': loss\n","    }\n","    torch.save(checkpoint, filepath)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T20:31:14.763806Z","iopub.status.busy":"2024-10-06T20:31:14.763488Z","iopub.status.idle":"2024-10-06T23:06:30.986940Z","shell.execute_reply":"2024-10-06T23:06:30.985817Z","shell.execute_reply.started":"2024-10-06T20:31:14.763775Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Iniciar treinamento\n","Epoch [1/35], d_loss: 1.8371, g_loss: 15.3380, val_loss: 14.6995\n","1380.361082315445\n"]},{"ename":"TypeError","evalue":"Invalid shape (3, 256, 256) for image data","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[24], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Plotando as três imagens lado a lado\u001b[39;00m\n\u001b[0;32m     67\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m---> 69\u001b[0m \u001b[43maxes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_image1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal Image 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     72\u001b[0m axes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(real_image2)\n","File \u001b[1;32mc:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\__init__.py:1486\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1491\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1492\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1493\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n","File \u001b[1;32mc:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:5895\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m-> 5895\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5896\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5898\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\image.py:729\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    728\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Isas_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\image.py:697\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[1;34m(A)\u001b[0m\n\u001b[0;32m    695\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[1;32m--> 697\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[0;32m    703\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n","\u001b[1;31mTypeError\u001b[0m: Invalid shape (3, 256, 256) for image data"]},{"data":{"image/png":"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","text/plain":["<Figure size 1500x500 with 3 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["from matplotlib import pyplot as plt\n","\n","# Instanciando o modelo GAN\n","gan = PaisagemGAN(generator=gene, discriminator=disc)\n","gan = gan.to(device)\n","gan.compile(g_loss=g_loss, d_loss=d_loss)\n","\n","# Número de épocas\n","num_epochs = 35\n","\n","\n","# Loop de treinamento\n","print(\"Iniciar treinamento\")\n","g_losses = []\n","d_losses = []\n","val_losses = []\n","for epoch in range(num_epochs):\n","    i = 0\n","    inicio = time.time()\n","    train_g_loss = 0.0\n","    train_d_loss = 0.0\n","    for batch in train_loader:\n","        real_images = normalize(batch[1].to(device))\n","        grey_images = normalize(batch[0].to(device))\n","        loss = gan.train_step(real_images, grey_images)\n","        i += 1\n","        train_g_loss += loss['g_loss']\n","        train_d_loss += loss['d_loss']\n","        real_images.cpu()\n","        grey_images.cpu()\n","    g_losses.append(train_g_loss/len(train_loader))\n","    d_losses.append(train_d_loss/len(train_loader))\n","    \n","    # Avaliação no conjunto de validação\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for val_batch in val_loader:\n","            val_real_images = normalize(val_batch[1].to(device))\n","            val_grey_images = normalize(val_batch[0].to(device))\n","            val_fake_images = gan.generator(val_grey_images)\n","            val_yhat_fake = gan.discriminator(val_fake_images)\n","            last_v_loss = g_loss(val_yhat_fake, val_fake_images, val_real_images)\n","            val_loss += last_v_loss.item()\n","            val_real_images.cpu()\n","            val_grey_images.cpu()\n","    \n","    val_loss /= len(val_loader)\n","    val_losses.append(val_loss)\n","    \n","    print(f\"Epoch [{epoch+1}/{num_epochs}], d_loss: {loss['d_loss']:.4f}, g_loss: {loss['g_loss']:.4f}, val_loss: {val_loss:.4f}\")\n","    fim = time.time()\n","    print(fim - inicio)\n","\n","    if (epoch+1) % 5 == 0 or (epoch) == 0:\n","        save_checkpoint(gan.generator, gan.g_opt, epoch, batch, loss, filepath='/kaggle/working/gen.pth')\n","        save_checkpoint(gan.discriminator, gan.d_opt, epoch, batch, loss, filepath='/kaggle/working/disc.pth')\n","\n","        # Desenhando evoluçao do treinamento\n","        # Convertendo as imagens geradas de 0-1 para 0-255\n","        gen_image = (loss['gen'][0].detach().cpu().numpy().transpose(1, 2, 0) * 255).astype('uint8')\n","\n","        # Convertendo batch[0] e batch[1] de 0-1 para 0-255\n","        real_image1 = (batch[0][0].detach().cpu().numpy().squeeze()*255).astype('uint8')\n","        real_image2 = (batch[1][0].detach().cpu().numpy().transpose(1, 2, 0)*255 ).astype('uint8')\n","\n","        # Plotando as três imagens lado a lado\n","        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","\n","        axes[0].imshow(real_image1)\n","        axes[0].set_title(\"Real Image 1\")\n","\n","        axes[1].imshow(real_image2)\n","        axes[1].set_title(\"Real Image 2\")\n","\n","        axes[2].imshow(gen_image)\n","        axes[2].set_title(f\"Generated Image at Epoch {epoch + 1}\")\n","\n","        for ax in axes:\n","            ax.axis('off')  # Remove os eixos\n","\n","        plt.show()\n","\n","        # Desenhando evolução da validação\n","        # Convertendo as imagens geradas de 0-1 para 0-255\n","        gen_image = (val_fake_images[0].detach().cpu().numpy().transpose(1, 2, 0) * 255).astype('uint8')\n","\n","        # Convertendo batch[0] e batch[1] de 0-1 para 0-255\n","        real_image1 = (val_grey_images[0].detach().cpu().numpy().squeeze()*255).astype('uint8')\n","        real_image2 = (val_real_images[0].detach().cpu().numpy().transpose(1, 2, 0)*255 ).astype('uint8')\n","\n","        # Plotando as três imagens lado a lado\n","        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","\n","        axes[0].imshow(real_image1, cmap='gray')\n","        axes[0].set_title(\"Real Image 1\")\n","\n","        axes[1].imshow(real_image2)\n","        axes[1].set_title(\"Real Image 2\")\n","\n","        axes[2].imshow(gen_image)\n","        axes[2].set_title(f\"Generated Image at Epoch {epoch + 1}\")\n","\n","        for ax in axes:\n","            ax.axis('off')\n","\n","        plt.show()\n","        \n","\n","print(\"Treinamento concluído!\")\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T23:06:30.989260Z","iopub.status.busy":"2024-10-06T23:06:30.988817Z","iopub.status.idle":"2024-10-06T23:06:30.995217Z","shell.execute_reply":"2024-10-06T23:06:30.994359Z","shell.execute_reply.started":"2024-10-06T23:06:30.989206Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[8.235966873168945, 8.104880719714695, 7.566074636247423, 7.727585665384928, 8.423988400565253, 7.777117167578803, 7.600813828574287, 7.583117983076307, 7.542600970798069, 7.549440728293525, 7.157434129714966, 7.406406195958455, 7.356189902623495, 7.649847014745077, 7.469735097885132, 7.582458469602797, 7.192126936382717, 7.205754068162706, 7.225237523184882, 7.289644463857015, 7.304311286078559, 7.558359824286567, 7.235311444600423, 7.610034396913316, 7.153232876459757, 7.370679791768392, 7.616324938668145, 7.421148390240139, 7.4200044631958, 7.139899656507704, 7.262660699420505, 7.165954822964139, 7.436315435833401, 7.2713229868147105, 7.191684701707628, 7.244011110729641, 7.344809532165527, 7.295557043287489, 7.43472646607293, 7.1065970738728845, 7.405883174472385, 7.237439563539293, 7.194000313017104, 7.264578527874416, 7.086265166600545, 7.073581435945299, 7.141798978381686, 7.300048107571072, 7.107680961820814, 7.106431743833753, 7.194342856936984, 7.327388991249932, 7.037262111239963, 7.102200444539388, 7.0660673989189995, 7.196796724531386, 7.163959630330404, 7.022278854582045, 7.499925639894274, 7.032919862535265, 7.119575696521335, 7.167373832066854, 7.060389227337307, 6.980704636043972, 6.951461505889893, 6.945335706075032, 7.099549288219876, 7.236232699288262, 7.0022558159298365, 7.226473167207506, 7.069405343797472, 6.9429716269175215, 6.754135836495293, 7.105240133073595, 6.967788680394491, 6.989535580741035, 7.206555986404419, 6.973185973697238, 7.175627507103814, 7.074738417731391, 7.076897817187839, 6.958323076036241, 6.932773235109117, 6.97226759062873, 7.048405191633436, 7.057992564307319, 7.04311498536004, 7.034586275948419, 7.054839362038506, 6.978870079252455, 6.9778260866800945, 7.090799628363715, 6.948207987679376, 7.069207270940145, 7.0137218899197045, 7.124985477659437, 6.963976589838664, 6.995318841934204, 6.971859720018175, 6.949353827370538]\n"]}],"source":["#torch.cuda.empty_cache()\n","print(val_losses)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-06T23:06:30.999530Z","iopub.status.busy":"2024-10-06T23:06:30.999167Z","iopub.status.idle":"2024-10-06T23:06:31.009719Z","shell.execute_reply":"2024-10-06T23:06:31.008585Z","shell.execute_reply.started":"2024-10-06T23:06:30.999486Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Losses saved to /kaggle/working/losses.csv\n"]}],"source":["import csv\n","\n","# Caminho do arquivo onde as perdas serão salvas\n","loss_file_path = '/kaggle/working/losses.csv'\n","\n","# Função para salvar as perdas em um arquivo CSV\n","def save_losses_to_csv(g_losses, d_losses, val_losses, file_path):\n","    with open(file_path, mode='w', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['Epoch', 'G Loss', 'D Loss', 'Validation Loss'])\n","        for epoch, (g, d, val_loss) in enumerate(zip(g_losses, d_losses, val_losses), start=1):\n","            writer.writerow([epoch, g, d, val_loss])\n","\n","# Salvar as perdas\n","save_losses_to_csv(g_losses, d_losses, val_losses, loss_file_path)\n","\n","print(f\"Losses saved to {loss_file_path}\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1036526,"sourceId":1913658,"sourceType":"datasetVersion"},{"datasetId":5827678,"sourceId":9562697,"sourceType":"datasetVersion"}],"dockerImageVersionId":30761,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
